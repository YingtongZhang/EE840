{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00b95b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare modules\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "from osgeo import gdal, ogr, gdalconst\n",
    "import os, cv2, glob, sys, fnmatch, time\n",
    "import math\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# from gdalconst import *\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from scipy import ndimage\n",
    "from skimage import filters as filters\n",
    "from skimage import transform\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Polygon\n",
    "from xml.dom import minidom\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17bb9484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn off the setting as copy warning for pandas dataframes\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b7c5b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_params(params):\n",
    "    \n",
    "    d = {}\n",
    "    \n",
    "    try:\n",
    "        with open(params) as f: input_vars = [line.split(\";\") for line in f]\n",
    "    except: \n",
    "        print('Problem reading parameter file:')\n",
    "        print(params) \n",
    "        sys.exit()\n",
    "    \n",
    "    for var in input_vars:\n",
    "        if len(var) == 2:\n",
    "            d[var[0].replace(\" \", \"\")] =\\\n",
    "                '{0}'.format(var[1].strip(\" \").replace(\"\\n\", \"\"))\n",
    "                # '\"{0}\"'.format(var[1].strip(\" \").replace(\"\\n\", \"\"))\n",
    "\n",
    "    print('Parameters read from:') \n",
    "    print(params + '\\n')\n",
    "\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f69c027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(txt, delimiter='\\t'):\n",
    "\n",
    "    df = pd.read_csv(txt, sep=delimiter, dtype={'file': object, 'frame': object})\n",
    "    df.columns = [c.replace(' ','') for c in df.columns]\n",
    "    df.replace(to_replace=' ', value='', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5907128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_directory(search_dir, search_str, file_ext, search_filters=[]):\n",
    "    ''' return a list of imgs in search_dir '''\n",
    "    \n",
    "    if not os.path.exists(search_dir):\n",
    "        print('Search directory does not exist:') \n",
    "        print(search_dir)\n",
    "        return None\n",
    "    \n",
    "    #imgs = glob.glob('{0}/*{1}*.{2}'.format(search_dir, search_str, file_ext))\n",
    "    if '[!' in search_str:\n",
    "        search_str, search_filters = search_str.split('[!')\n",
    "        search_filters = [s.strip().replace(']', '') for s in search_filters.split(',')]\n",
    "    \n",
    "    imgs = []\n",
    "    for root, dirs, files in os.walk(search_dir):\n",
    "        matched_files = fnmatch.filter(files, '*%s*.%s' % (search_str, file_ext))\n",
    "        imgs.extend([os.path.join(root, f) for f in matched_files])\n",
    "    \n",
    "    if len(search_filters):\n",
    "        imgs = [i for i in imgs for f in search_filters if f not in i]\n",
    "        \n",
    "    if len(imgs) == 0: \n",
    "        print('No images found in:\\n%s' % search_dir)\n",
    "        import pdb; pdb.set_trace()\n",
    "\n",
    "    return imgs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df595008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rgb(txt, sep='\\t'):\n",
    "    \n",
    "    r = {}\n",
    "    g = {}\n",
    "    b = {}\n",
    "    \n",
    "    if txt.endswith('qml'):\n",
    "        return qml_to_rgb(txt)\n",
    "    \n",
    "    try:\n",
    "        with open(txt) as f: lines = [line for line in f][1:]\n",
    "    except: \n",
    "        raise IOError('Problem reading RGB file:\\n%s' % txt)   \n",
    "    \n",
    "    try:\n",
    "        for line in lines:\n",
    "            rgb = line.strip('\\n').split(sep)\n",
    "            # If single band, rgb = value, r, g, b\n",
    "            if len(rgb) == 4:\n",
    "                value = int(rgb[0])\n",
    "                r[value] = int(rgb[1])\n",
    "                g[value] = int(rgb[2])\n",
    "                b[value] = int(rgb[3])\n",
    "            \n",
    "            # If multi-band, rgb = rvalue, r, gvalue, g, bvalue, b\n",
    "            elif len(rgb) == 6:\n",
    "                r[int(rgb[0])] = int(rgb[1])\n",
    "                g[int(rgb[2])] = int(rgb[3])\n",
    "                b[int(rgb[4])] = int(rgb[5])\n",
    "            else:\n",
    "                raise RuntimeError(r'Invalid number of RGB values specified. Check that the rgb_sep matches the delimiter of the RGB text file.\\nrgb_sep: %s\\n' % sep)\n",
    "    \n",
    "    except:\n",
    "        print('Check the format of ')\n",
    "        \n",
    "    return dict(zip(['r','g','b'], [r, g, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a7c13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_rgb(d):\n",
    "    ''' Returns True if d is a valid rgb dict, otherwise returns False'''\n",
    "    \n",
    "    if d == False or d['r'] == {} or d['g'] == {} or d['b'] =={}:\n",
    "        raise RuntimeError('Invalid rgb dict: %s' % d)\n",
    "    \n",
    "    keys = d['r'].keys()\n",
    "    unique_keys, key_counts = np.unique(keys, return_counts=True)\n",
    "    duplicate_keys = unique_keys[key_counts > 1]\n",
    "    if len(duplicate_keys) > 0:\n",
    "        raise KeyError('Duplicate stops found in RGB text: %s' % ','.join(duplicate_keys))\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "322c0385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_director(direct_txt, imgs, xy_txt, img_size, m_per_frame, fps, fade_time=2, min_last_year=0.75):\n",
    "    \n",
    "    # Make df from lat/long text file\n",
    "    min_last_year = int(min_last_year * fps) # min last year is in seconds\n",
    "    xy = read_txt(xy_txt)\n",
    "    print(xy)\n",
    "\n",
    "    #with open(direct_txt, 'w') as txt:\n",
    "    # Write columns of director\n",
    "    #txt.write('file\\tx\\ty\\tmov_rows\\tmov_cols\\tarray_rows\\tarray_cols\\treplace_method\\tsearch_string\\tyear\\tframe\\tin_extent\\tdataband\\thillshade\\n')\n",
    "\n",
    "    # Get max number of leading 0s for numeric tag of the frame images \n",
    "    tag_len = -6\n",
    "    \n",
    "    frame = 0\n",
    "    i = 0\n",
    "    x = 0\n",
    "    y = 0\n",
    "    x2 = 1\n",
    "    y2 = 1\n",
    "    fade_frames = []\n",
    "    director = []\n",
    "\n",
    "    '''speed_funcs = {'none': 'd = t',\n",
    "               'start': 'd = -1.16*((t - 0.6687)**5 - t) - .1551',\n",
    "               'end': 'd = -1.16*((t-.3313)**5 - t) - .00485',\n",
    "               'startend': 'd = (6*(t**5) - 15*(t**4) + 10*(t**3))'}\n",
    "    zoom_funcs = {'none': 'ht = 1',\n",
    "                'start': 'ht = -(6*t**5) + (15*t**4) - (10*t**3) + 1',\n",
    "                'end': 'ht = (6*t**5) - (15*t**4) + (10*t**3)',\n",
    "                'startend': 'ht = 16 * ((t**4) - 2*(t**3) + (t**2))',#,\n",
    "                'constant': 'ht = 1'}'''\n",
    "    speed_funcs = {'none':     lambda t: t,\n",
    "                   'start':    lambda t: -1.16*((t - 0.6687)**5 - t) - .1551,\n",
    "                   'end':      lambda t: -1.16*((t-.3313)**5 - t) - .00485,\n",
    "                   'startend': lambda t: 6*(t**5) - 15*(t**4) + 10*(t**3),\n",
    "                   'accel':    lambda t: -1.16*((t - 0.6687)**5 - t) - .1551,\n",
    "                   'decel':    lambda t: -1.16*((t-.3313)**5 - t) - .00485,\n",
    "                   'fastslow': lambda t: 6*(t**5) - 15*(t**4) + 10*(t**3)} \n",
    "    \n",
    "    zoom_funcs = {'none':    lambda t: 1,\n",
    "                  'start':   lambda t: -(6*t**5) + (15*t**4) - (10*t**3) + 1,\n",
    "                  'end':     lambda t: (6*t**5) - (15*t**4) + (10*t**3) + 1,\n",
    "                  'startend': lambda t: 16 * ((t**4) - 2*(t**3) + (t**2)) + 1,\n",
    "                  'in':      lambda t: -(6*t**5) + (15*t**4) - (10*t**3) + 1,\n",
    "                  'out':     lambda t: (6*t**5) - (15*t**4) + (10*t**3) + 1,\n",
    "                  'outin':   lambda t: 16 * ((t**4) - 2*(t**3) + (t**2)) + 1}\n",
    "\n",
    "    smooth_curve = 0\n",
    "    # year_f = xy.ix[0, 'year_start'] // ix is deprecated\n",
    "    year_f = xy.loc[0, 'year_start']\n",
    "    band_f = 1\n",
    "    hillshade = 0\n",
    "    # For each route point, write a new line with an interpolated (x,y) location\n",
    "    for i, row in xy.iterrows():\n",
    "        # Get params from rows\n",
    "        if smooth_curve == 0:\n",
    "            x = row['x_start']\n",
    "            y = row['y_start']\n",
    "            x2 = row['x_end']\n",
    "            y2 = row['y_end']\n",
    "     \n",
    "        year_start = row['year_start']\n",
    "        if year_start != 0 and year_f < year_start:\n",
    "            year_f = year_start # For switch from no-year image back to timeseries\n",
    "        year_end = row['year_end']\n",
    "        \n",
    "        by_file = row['by_file']\n",
    "        band_start = row['band_start']\n",
    "        if not by_file:\n",
    "            band_yr_dif = year_start - band_start\n",
    "            band_f = year_f - band_yr_dif\n",
    "        else:\n",
    "            band_f = band_start\n",
    "     \n",
    "        zoom = row['zoom']\n",
    "        ht_scale = row['height_scale']\n",
    "        accel = row['acceleration']\n",
    "        spd_scale = row['speed_scale']\n",
    "        #smooth_curve = row['smooth_curve']\n",
    "        search_str = row['search_string'].split('[!')[0]#in case a not filter was given\n",
    "        replace_method = row['replace_method']\n",
    "        nframes = row['nframes']\n",
    "        scene_rt = row['scenes_per_sec']/float(fps)\n",
    "        if 'hillshade' in xy.columns:\n",
    "        \thillshade = row.hillshade\n",
    "        \n",
    "        try:\n",
    "            delta_x = int(x2 - x)\n",
    "            delta_y = int(y2 - y)\n",
    "        except:\n",
    "            import pdb; pdb.set_trace()\n",
    "        #tval = tval_dic[accel]\n",
    "        fade_spd = fps * fade_time\n",
    "\n",
    "        # Calculate n frames for this segment if the start and end \n",
    "        #   locations are different\n",
    "        if x2 != x or y2 != y:\n",
    "            nframes = int((((delta_x ** 2 + delta_y ** 2) ** .5)/m_per_frame)/spd_scale)\n",
    "        \n",
    "        \n",
    "        times = [float(j)/nframes for j in range(nframes)]\n",
    "        # distances = map(speed_funcs[accel], times)\n",
    "        # heights = map(zoom_funcs[zoom], times)\n",
    "        distances = list(map(speed_funcs[accel], times))\n",
    "        heights = list(map(zoom_funcs[zoom], times))\n",
    "        last_year_count = 0\n",
    "\n",
    "        for j in range(int(nframes)):\n",
    "            t = times[j]#float(j)/nframes# * tval\n",
    "            \n",
    "            # Calculate x and y for this frame\n",
    "            #exec(speed_funcs[accel]) # Calculate value of d\n",
    "            d = distances[j] #apply(speed_funcs[accel], [t])\n",
    "            this_x = x + d * delta_x\n",
    "            this_y = y + d * delta_y\n",
    "    \n",
    "            # Calculate altitude\n",
    "            mov_rows, mov_cols = img_size\n",
    "            #exec(zoom_funcs[zoom]) #Calculate ht\n",
    "            #ht *= ht_scale\n",
    "            ht = heights[j] * ht_scale #apply(zoom_funcs[zoom], [t]) * ht_scale\n",
    "            a_rows = int((mov_rows + ht * mov_rows))# * ht_scale)\n",
    "            a_cols = int((mov_cols + ht * mov_cols))# * ht_scale)\n",
    "            #a_rows = max(1, int(mov_rows * ht)) # in case ht is 0\n",
    "            #a_cols = max(1, int(mov_cols * ht))\n",
    "            #a_rows = int(mov_rows * ht * ht_scale)        \n",
    "            #a_cols = int(mov_cols * ht * ht_scale)\n",
    "            \n",
    "            t_round = np.round(t,2)\n",
    "            t_remain =  t_round * 100 % 20\n",
    "            if t_remain == 0 or t_round == 1 or (x2 == x and y2 == y):\n",
    "                in_extent = 1\n",
    "            else: \n",
    "                in_extent = 0\n",
    "            \n",
    "            # Write the line in the director\n",
    "            # import pdb; pdb.set_trace()\n",
    "            if year_start != 0 or year_end != 0:\n",
    "                band = int(round(band_f, 0))\n",
    "                try:\n",
    "                    #year = int(round(year_f, 0))\n",
    "                    year = int(year_f)\n",
    "                    if by_file:\n",
    "                        # for img in fnmatch.filter(imgs, '*%s*' % search_str):\n",
    "                            # if str(year) in img:\n",
    "                                # imgfile = ','.join(img)\n",
    "                        imgfile = ','.join([img for img in fnmatch.filter(imgs, '*%s*' % search_str) if str(year) in img])\n",
    "                    else:\n",
    "                        imgfile = ','.join(fnmatch.filter(imgs, '*%s*' % search_str))\n",
    "                    if imgfile == '': import pdb; pdb.set_trace()\n",
    "                    \n",
    "                except:\n",
    "                    print('No image in list with {0} and {1} in filename'.format(search_str, year))\n",
    "                    continue            \n",
    "            else:\n",
    "                try:\n",
    "                    imgfile = [img for img in imgs if search_str in img][0]\n",
    "                    year = 0\n",
    "                    band = band_start\n",
    "                except:\n",
    "                    print('No image in list with %s in filename' % search_str)\n",
    "                    continue            \n",
    "            frame_str = '0000000%s' % frame\n",
    "\n",
    "            director.append({'file': imgfile,\n",
    "                              'x': this_x,\n",
    "                              'y': this_y,\n",
    "                              'mov_rows': mov_rows,\n",
    "                              'mov_cols': mov_cols,\n",
    "                              'array_rows': a_rows,\n",
    "                              'array_cols': a_cols,\n",
    "                              'replace_method': replace_method,\n",
    "                              'search_string': row.search_string,\n",
    "                              'year': year,\n",
    "                              'frame': frame_str[tag_len:], \n",
    "                              'in_extent': in_extent,\n",
    "                              'databand': band,\n",
    "                              'hillshade': hillshade\n",
    "                              })\n",
    "\n",
    "            # Incriment the year and the frame    \n",
    "            frame += 1\n",
    "            \n",
    "            # If this is the last year in the time series, show it for a specified time before restarting the time series\n",
    "            if year == year_end:\n",
    "                if last_year_count < min_last_year:\n",
    "                    last_year_count += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    last_year_count = 0 #Start the count over\n",
    "                    year_f = year_end + 1.0 # Start the year series over\n",
    "            \n",
    "            year_f += scene_rt\n",
    "            if not by_file: # Increment the band if the timeseries is by band\n",
    "                band_f += scene_rt\n",
    "            if year_f > year_end + 1: \n",
    "                year_f = year_start\n",
    "                band_f = band_start\n",
    "        # Make room in the frame tag sequence to interpolate if necessary\n",
    "        #   and record the frame number\n",
    "        # if i < len(xy) - 1 and row.search_string != xy.ix[i + 1,'search_string']:\n",
    "        if i < len(xy) - 1 and row.search_string != xy.loc[i + 1,'search_string']:\n",
    "            frame += fade_spd - 1\n",
    "            fade_frames.append(frame_str[tag_len:])    \n",
    "\n",
    "    pd.DataFrame(director).to_csv(direct_txt, sep='\\t', index=False)\n",
    "    \n",
    "    return fade_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6678df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coords(shp, return_fid=False):\n",
    "    ''' \n",
    "    Return a list of lists of the projected coordinates of vertices of shp. \n",
    "    Each list represents a feature in the dataset.\n",
    "    '''    \n",
    "    ds = ogr.Open(shp)\n",
    "    lyr = ds.GetLayer()\n",
    "    \n",
    "    # For each feature, get geometry ref.\n",
    "    coords = []\n",
    "    for i in range(lyr.GetFeatureCount()): \n",
    "        feature = lyr.GetFeature(i)\n",
    "        geom = feature.GetGeometryRef()\n",
    "        #print geom.ExportToWkt()\n",
    "        \n",
    "        # Get points for geometry ref\n",
    "        #pts = [[pt.GetPoints() for pt in geom.GetGeometryRef(i)] for i in range(geom.GetGeometryCount())]\n",
    "        for j in range(geom.GetGeometryCount()):\n",
    "            this_g = geom.GetGeometryRef(j) #If geom is a Multipolgyon\n",
    "            wkt = this_g.ExportToWkt()\n",
    "            pts_list = wkt.replace('POLYGON','').replace('LINEARRING','').replace('(','').replace(')','').strip().split(',')\n",
    "            x = [float(p.split(' ')[0]) for p in pts_list]\n",
    "            y = [float(p.split(' ')[1]) for p in pts_list]\n",
    "            pts = zip(x,y)\n",
    "            if return_fid:\n",
    "                fid = feature.GetFID()\n",
    "                pts = fid, pts\n",
    "            coords.append(pts)\n",
    "    #this_lyr = None\n",
    "    #geom = None\n",
    "    #this_g = None\n",
    "    \n",
    "    # Need the bounds for calculating relative xy in image coords\n",
    "    bounds = lyr.GetExtent()\n",
    "    ds = None\n",
    "\n",
    "    return coords, bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5dd6f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shp_to_array(ds_coords, ds_extent, max_size, out_dir, fill='0.7', line_color='0.5', line_width=1.0, dpi=72, pad=None, rotation=None, out_file='extent.png'):\n",
    "    '''\n",
    "    Return an RGB array from of polygons using vertices from ds_coords. \n",
    "    '''\n",
    "\n",
    "    #buffer_size = int(dpi * pad)\n",
    "    x_min, x_max, y_min, y_max = ds_extent\n",
    "    x_size = float(x_max - x_min)\n",
    "    y_size = float(y_max - y_min)\n",
    "    max_prj_size = max(x_size, y_size)\n",
    "    #buffer_size = int(max_size * img_buffer)\n",
    "    n_cols_img = int(round(x_size/max_prj_size * max_size * 1.3))#adjust for mpl shrinking\n",
    "    n_rows_img = int(round(y_size/max_prj_size * max_size * 1.3))\n",
    "    aspect_ratio = float(n_cols_img)/n_rows_img\n",
    "    rows = n_rows_img #- buffer_size * 2\n",
    "    cols = int(round(rows * aspect_ratio, 0))\n",
    "    \n",
    "    x_scale = 1/x_size\n",
    "    y_scale = 1/y_size\n",
    "    #fig, ax = plt.subplot(nrows=rows, cols, 1)\n",
    "    sub_params = matplotlib.figure.SubplotParams(wspace=0,hspace=0)\n",
    "    fig = plt.figure(figsize=(cols/dpi, rows/dpi), dpi=dpi, facecolor='k')#, tight_layout={'pad':pad})\n",
    "\n",
    "    sub = fig.add_subplot(1, 1, 1, facecolorreplace_with_color='k', frame_on=False)#, axes=a)\n",
    "    \n",
    "    sub.axes.get_yaxis().set_visible(False)\n",
    "    sub.axes.get_xaxis().set_visible(False)\n",
    "    \n",
    "    patches = []\n",
    "    ds_coords = [c for c in ds_coords if c != None]\n",
    "    for feature in ds_coords:\n",
    "        img_coords = [(((pt[0] - x_min) * x_scale), (pt[1] - y_min) * y_scale) for pt in feature]\n",
    "\n",
    "        ar_coords = np.array(img_coords)\n",
    "        poly = matplotlib.patches.Polygon(ar_coords)\n",
    "        patches.append(poly)\n",
    "\n",
    "    p = PatchCollection(patches, cmap=matplotlib.cm.jet, color=fill, lw=line_width, edgecolor=line_color)\n",
    "    sub.add_collection(p)\n",
    "    out_img = os.path.join(out_dir, out_file)\n",
    "    plt.savefig(out_img, dpi=dpi, facecolor=fig.get_facecolor())\n",
    "    \n",
    "    plt.cla()\n",
    "    p_a = PatchCollection(patches, cmap=matplotlib.cm.jet, color='k', lw=.1, edgecolor='k')\n",
    "    sub.add_collection(p_a)\n",
    "    alpha_img = out_img.replace('.png', '_alpha.png')\n",
    "    plt.savefig(alpha_img, dpi=dpi, facecolor='w')#'''\n",
    "    \n",
    "    # read it back into memory\n",
    "    ar = cv2.imread(out_img).astype(np.int32)\n",
    "    alpha = 1 - cv2.imread(alpha_img)/255.0\n",
    "    if rotation:\n",
    "        # Shouldn't rotate here because plotting points will be off\n",
    "        ar = rotate_array(ar, rotation, 0)\n",
    "        alpha = rotate_array(alpha, rotation, 1.0)\n",
    "    # clip it because tight_layout() is stupid and changes aspect ratio \n",
    "    ar, inds = clip_nodata_edges(ar, 0, return_inds=True)\n",
    "    alpha = alpha[inds[0] : inds[1], inds[2] : inds[3]]\n",
    "    #import pdb; pdb.set_trace()\n",
    "    \n",
    "    # buffer it\n",
    "    offset = None\n",
    "    if pad is not None and pad > 0:\n",
    "        nrows, ncols, _ = ar.shape\n",
    "        if isinstance(pad, float):\n",
    "            row_buf = [int(nrows * pad/2)] * 2\n",
    "            col_buf = [int(ncols * pad/2)] * 2\n",
    "        else:# len(list(pad)) == 4:\n",
    "            sizes = np.array([nrows, nrows, ncols, ncols])\n",
    "            floats = np.array([isinstance(p, float) for p in pad])\n",
    "            pad = np.array(pad)\n",
    "            pad[floats] = sizes[floats] * pad[floats]\n",
    "            row_buf = pad[:2].astype(int)\n",
    "            col_buf = pad[2:].astype(int)\n",
    "        buf_nrows = nrows + sum(row_buf)\n",
    "        buf_ncols = ncols + sum(col_buf)\n",
    "        # buffer ar\n",
    "        ar_out = np.zeros((buf_nrows, buf_ncols, 3))\n",
    "        ar_out[row_buf[0]:-row_buf[1], col_buf[0]:-col_buf[1]] = ar\n",
    "        ar = ar_out.copy()\n",
    "        # buffer alpha\n",
    "        ar_out = np.zeros((buf_nrows, buf_ncols, 3))\n",
    "        ar_out[row_buf[0]:-row_buf[1], col_buf[0]:-col_buf[1]] = alpha\n",
    "        alpha = ar_out.copy()\n",
    "        offset = np.concatenate([row_buf, col_buf])\n",
    "    \n",
    "    return ar, alpha, offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5074557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_nodata_edges(ar, nodata, clip_rows=True, clip_cols=True, buffer_size=None, fill_val=0, return_inds=False):\n",
    "    ''' Remove all rows and columns that are all nodata'''\n",
    "    \n",
    "    if np.all(ar == nodata):\n",
    "        warnings.warn('\\nall array values == fill_val %d' % nodata)\n",
    "        return ar\n",
    "    \n",
    "    shape = list(ar.shape)\n",
    "    if len(shape) == 2:\n",
    "        ar = ar.reshape(shape[0], shape[1], 1)\n",
    "    first_row, first_col = 0, 0 # initialize in case clip_rows/cols is false\n",
    "    last_row, last_col = shape[:2]\n",
    "    \n",
    "    # Get row indices\n",
    "    if clip_rows:\n",
    "        row_mask = np.all(~np.all(ar == nodata, axis=1), axis=1)\n",
    "        keep_rows = np.arange(shape[0])[row_mask]\n",
    "        first_row = keep_rows.min()\n",
    "        last_row = keep_rows.max() + 1\n",
    "        shape[0] = row_mask.size\n",
    "    # Get col indices\n",
    "    if clip_cols:\n",
    "        col_mask = np.all(~np.all(ar == nodata, axis=0), axis=1)\n",
    "        keep_cols = np.arange(shape[1])[col_mask]\n",
    "        first_col = keep_cols.min()\n",
    "        last_col = keep_cols.max() + 1\n",
    "        shape[1] = keep_cols.size\n",
    "    \n",
    "    # Clip\n",
    "    clipped = ar[first_row:last_row, first_col:last_col]\n",
    "    \n",
    "    # Add a couple of nodata rows back in for a visual buffer\n",
    "    if buffer_size:\n",
    "        shape = clipped.shape\n",
    "        if isinstance(buffer_size, float): \n",
    "            buffer_size = int(max(shape) * buffer_size) * 2\n",
    "        out_shape = [shape[0] + buffer_size, shape[1] + buffer_size] + [dim for dim in shape[2:]]\n",
    "        ar_buf = np.full(out_shape, fill_val, dtype=clipped.dtype)\n",
    "        ar_buf[buffer_size : buffer_size + shape[0], buffer_size : buffer_size + shape[1]] = clipped\n",
    "        clipped = ar_buf\n",
    "        first_row -= buffer_size/2\n",
    "        first_col -= buffer_size/2\n",
    "        last_row += buffer_size/2\n",
    "        last_col += buffer_size/2\n",
    "    \n",
    "    if return_inds:\n",
    "        return clipped, (first_row, last_row, first_col, last_col)\n",
    "    else:\n",
    "        return clipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40dcf906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_extent_offsets(df, ds_extent, pt_size, ar_shape, extent_offset):\n",
    "    '''\n",
    "    Calculate pixel offsets in place for plotting points in the extent\n",
    "    indicator. This is a seperate function to insulate the lambda expression\n",
    "    from the exec call in the main() function (throws an error for some reason\n",
    "    when it's in main())\n",
    "    '''\n",
    "    \n",
    "    # Calculate x_scale and y_scale (redundant? already calculated in shp_to_array)\n",
    "    x_min, x_max, y_min, y_max = ds_extent\n",
    "    delta_x = x_max - x_min\n",
    "    delta_y = y_max - y_min\n",
    "    x_scale = float(ar_shape[1] - sum(extent_offset[2:]))/delta_x\n",
    "    y_scale = float(ar_shape[0] - sum(extent_offset[:2]))/delta_y\n",
    "        \n",
    "    #Calculate row and column offsets for each point\n",
    "    df.loc[:,'x_off'] = df.loc[:,'x'].map(lambda z: abs(x_min - z) * x_scale + pt_size/2)\n",
    "    df.loc[:,'y_off'] = df.loc[:,'y'].map(lambda z: abs(y_max - z) * y_scale + pt_size/2)\n",
    "    #import pdb; pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4228c285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_text(txt_str, font_path, font_size, img_size=None, bg_color=(255, 255, 255), txt_color=(0, 0, 0)):\n",
    "    \n",
    "    thisfont = ImageFont.truetype(font_path, font_size)\n",
    "    txt_size = thisfont.getsize(txt_str)\n",
    "    cols, rows = txt_size\n",
    "    \n",
    "    img = Image.new('RGB', (cols, rows), color=bg_color)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.text((0, 0), txt_str, font=thisfont, fill=txt_color)\n",
    "    img.resize((cols, rows), Image.ANTIALIAS)\n",
    "    \n",
    "    return np.asarray(img), txt_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4052142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_dic(years, font_path, rows, cols, ul_y=None, ul_x=None, ctr_x=None, font_size=None):\n",
    "    \n",
    "    dic = {}\n",
    "    if not font_size:\n",
    "        font_size = int(rows/15)\n",
    "    if ul_y is None:\n",
    "        ul_y = rows - rows/5\n",
    "    \n",
    "    txt_rows = []\n",
    "    txt_cols = []\n",
    "    \n",
    "    for year in years:\n",
    "        text, txt_size = draw_text(str(year), font_path, font_size)\n",
    "\n",
    "        txt_rows.append(txt_size[1])\n",
    "        txt_cols.append(txt_size[0])\n",
    "        \n",
    "        ar = np.full((rows, cols, 3), 255)\n",
    "        if ul_x is None:\n",
    "            if ctr_x is not None: # ctr_x should be center of text\n",
    "                ul_x = ctr_x - txt_size[0]/2\n",
    "            else:\n",
    "                ul_x = cols/2 - txt_size[0]/2 # center it in the array\n",
    "        # import pdb; pdb.set_trace()\n",
    "        ar[int(ul_y):int(ul_y) + txt_size[1], int(ul_x):int(ul_x) + txt_size[0]] = text\n",
    "        dic[year] = ar\n",
    "    \n",
    "    max_size = max(txt_rows), max(txt_cols)\n",
    "    \n",
    "    #For non-timeseries images\n",
    "    dic[0] = 0\n",
    "    \n",
    "    return dic, max_size, ul_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d583eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frame(frame_info, bands, rgb_dics, hillshade_path, hs_opacity, extent_shp, extent_info, burn_coords, burn_extent, txt_dict, txt_size, txt_uly, legends=None, legend_inds=None, blur=7, extent_position='lower right', ctr_coords=None, gamma=.7, gamma_lookup=None):\n",
    "    \n",
    "    ctr_x = frame_info['x']\n",
    "    ctr_y = frame_info['y']\n",
    "    mov_rows = frame_info['mov_rows']\n",
    "    mov_cols = frame_info['mov_cols']\n",
    "    array_rows = frame_info['array_rows']\n",
    "    array_cols = frame_info['array_cols']\n",
    "    replace_method = frame_info['replace_method'].strip()\n",
    "    search_str = frame_info['search_string'].strip()\n",
    "    year = frame_info['year']\n",
    "    databand = frame_info['databand']\n",
    "\n",
    "    # Replace values with rgb\n",
    "    if replace_method == 'm':\n",
    "        array = {}\n",
    "        files = [f for f in frame_info.file.split(',')]\n",
    "        dic = dict([[j.replace(' ', '') for j in i.split(':')] for i in bands.split(',')])\n",
    "        for c in dic:\n",
    "            try:\n",
    "                file_path = fnmatch.filter(files, dic[c])[0]\n",
    "            except IndexError:\n",
    "                raise IOError(\"Can't find file for band %s with search str %s in these files: \\n\\t%s\" % (c, dic[c], '\\n\\t'.join(files)))\n",
    "            ds = gdal.Open(file_path)\n",
    "            xsize = ds.RasterXSize\n",
    "            ysize = ds.RasterYSize\n",
    "            tx = ds.GetGeoTransform()\n",
    "            array[c] = extract_kernel(ds, ctr_x, ctr_y, array_rows, array_cols, tx, xsize, ysize, databand=databand)\n",
    "            ''' put file in rgb_txt '''\n",
    "        rgb_array = get_rgb_array(array, rgb_dics[search_str], 'm')\n",
    "    else:\n",
    "        ds = gdal.Open(frame_info.file)\n",
    "        xsize = ds.RasterXSize\n",
    "        ysize = ds.RasterYSize\n",
    "        tx = ds.GetGeoTransform()\n",
    "        array = extract_kernel(ds, ctr_x, ctr_y, array_rows, array_cols, tx, xsize, ysize, databand=databand)\n",
    "        rgb_array = get_rgb_array(array, rgb_dics[search_str], replace_method)\n",
    "\n",
    "    # If a shaded relief raster is given in the params file, extract \n",
    "    #   the hillshade array and blend it with the rgb_array\n",
    "    if hillshade_path and frame_info.hillshade:\n",
    "        ds_hs = gdal.Open(hillshade_path)\n",
    "        tx_hs = ds_hs.GetGeoTransform()\n",
    "        hs_xsize = ds_hs.RasterXSize\n",
    "        hs_ysize = ds_hs.RasterYSize\n",
    "        try:\n",
    "            hs_array = extract_kernel(ds_hs, ctr_x, ctr_y, array_rows, array_cols, tx_hs, hs_xsize, hs_ysize)\n",
    "        except:\n",
    "            raise RuntimeError('Problem getting hillshade array:\\n%s'\\\n",
    "            % hillshade_path)\n",
    "        rgb_array =  blend_arrays(rgb_array, hs_array, opacity=hs_opacity)\n",
    "        if gamma:\n",
    "            arange = np.arange(256, dtype=np.uint8)\n",
    "            gamma_lookup = dict(zip(arange, gamma_stretch(arange, gamma)))\n",
    "            rgb_array = replace_with_color(rgb_array.astype(np.uint8), gamma_lookup)\n",
    "                        \n",
    "        #rgb_array = blend_multiply(rgb_array, hs_array * (1 - hs_opacity/100.0) + hs_opacity)\n",
    "        #rgb_array[hs_array == 0] = 0\n",
    "        #rgb_array = pan_sharpen(rgb_array, hs_array)\n",
    "    #zoom = float(array_rows)/mov_rows\n",
    "    if burn_coords:\n",
    "        rgb_array = burn_shp_to_array(rgb_array, burn_coords, burn_extent, ctr_x, ctr_y, tx[1], os.path.dirname(frame_info.basepath))\n",
    "    \n",
    "    # Resample if zoomed out\n",
    "    if array_rows != mov_rows or array_cols != mov_cols:\n",
    "        rgb_array = transform.resize(rgb_array, (mov_rows, mov_cols), order=3)\n",
    "    \n",
    "    # Add legend\n",
    "    if legends is not None and search_str in legends:\n",
    "        legend, alpha = legends[search_str]\n",
    "        l_nrows, l_ncols, _ = legend.shape\n",
    "        # blur and darken background\n",
    "        sidebar = rgb_array[:, legend_inds[2]:legend_inds[3]]\n",
    "        _, _, buf_inds = get_overlay_inds(extent_position, np.diff(legend_inds)[[0,2]], (mov_rows, mov_cols), bg_buffer=max(5, l_ncols/120))\n",
    "        buf_inds[0] = 0\n",
    "        buf_inds[1] = mov_rows\n",
    "        background = blur_background(sidebar, blur, buf_inds, bg_opacity=.4)\n",
    "        # add legend\n",
    "        l_ur = legend_inds[0] + (legend_inds[1] - legend_inds[0])/2 - l_nrows/2\n",
    "        l_lc = (legend_inds[3] - legend_inds[2])/2 - l_ncols/2\n",
    "        legend_bg = background[l_ur:l_ur + l_nrows, l_lc:l_lc + l_ncols]\n",
    "        background[l_ur:l_ur + l_nrows, l_lc:l_lc + l_ncols] = blend_arrays(legend_bg, legend, alpha)\n",
    "        rgb_array[:, legend_inds[2]:legend_inds[3]] = background\n",
    "        blur = None\n",
    "        ctr_coords = legend_inds[2:]\n",
    "\n",
    "    if extent_shp:\n",
    "        ds_extent, base_ar, pt_size, pt_mask, df_extent, pt_time, extent_ar, alpha_ar, extent_position, rot = extent_info\n",
    "        #Calculate the alpha value numerators on the fly\n",
    "        ind = df_extent.index[-1]\n",
    "        df_extent['alpha'] = float(ind) - df_extent.index\n",
    "        extent_ar = plot_extent_pts(ds_extent, base_ar, pt_size, pt_mask, df_extent, pt_time)\n",
    "        try:    \n",
    "            extent_position.lower()\n",
    "        except: # extent_position wasn't specified\n",
    "            extent_position=None\n",
    "        rgb_array = add_extent_to_frame(rgb_array, extent_ar, alpha_ar, extent_position, rotation=rot, blur=blur, ctr_coords=ctr_coords)\n",
    "\n",
    "    ds_hs = None\n",
    "    \n",
    "    # Add the halo for the text to the array\n",
    "    if year > 0 and legends is None: # if legend, text goes above it so no halo\n",
    "        rgb_array = make_halo(rgb_array, txt_size, txt_uly)\n",
    "    \n",
    "    save_frame(rgb_array, txt_dict[year], frame_info.basepath, frame_info.frame)\n",
    "    #import pdb; pdb.set_trace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7da37963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def par_make_frame(args):\n",
    "    ''' Helper function for making frames in parallel'''\n",
    "    make_frame(*args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f2a23f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3c3663",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50ff2b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frame(rgb_array, txt_array, basepath, frame):\n",
    "   \n",
    "    if type(txt_array) != int:\n",
    "        rgb_array = blend_arrays(rgb_array, np.full(txt_array.shape, 255, dtype=np.int32), 1 - txt_array/255.0)\n",
    "    \n",
    "    path = basepath + '_{0}.png'.format(str(frame).replace(' ', ''))\n",
    "    cv2.imwrite(path, rgb_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f043b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_complex_params(var_dic):\n",
    "    \n",
    "    search_strs = var_dic['search_strs'].split(',')\n",
    "    \n",
    "    params_list = 'search_dirs', 'rgb_txts', 'file_extensions', 'bands'\n",
    "    for p in params_list:\n",
    "        var_dic[p] = str(dict([[this_pair for this_pair in pair.strip().split(' ')] for pair in var_dic[p].replace('\"', '').split(',')]))\n",
    "\n",
    "    return var_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "288d85dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qml_to_rgb(qml):\n",
    "    ''' Make rgb dictionaries from a QML style doc'''\n",
    "    \n",
    "    xml_doc = minidom.parse(qml)\n",
    "    items = xml_doc.getElementsByTagName('item')\n",
    "    if len(items) == 0:\n",
    "        items = xml_doc.getElementsByTagName('paletteEntry')\n",
    "    values = [int(i.attributes.get('value').value) for i in items]\n",
    "    colors = np.array([matplotlib.colors.hex2color(i.attributes.get('color').value) for i in items])\n",
    "    df = pd.DataFrame(np.uint8(colors * 255), columns=['r','g','b'], index=values)\n",
    "    \n",
    "    return df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e27417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_bezier(t, p0, p1, p2):\n",
    "    '''\n",
    "    Return the x and y coordinates at t of a bezier curve defined by p0, p1, \n",
    "    and p2 - start, contol, and end points, respectively\n",
    "    '''\n",
    "    t2 = t ** 2\n",
    "    mt = 1 - t\n",
    "    mt2 = mt ** 2\n",
    "    x = p0[0] * mt2 + p1[0] * 2 * mt * t + p2[0] * t2\n",
    "    y = p0[1] * mt2 + p1[1] * 2 * mt * t + p2[1] * t2\n",
    "    \n",
    "    '''# Calculate q0\n",
    "    delta_x1 = (p1[0] - p0[0]) * t\n",
    "    delta_y1 = (p1[1] - p0[1]) * t\n",
    "    q0_x = p0[0] + delta_x1\n",
    "    q0_y = p0[1] + delta_y1\n",
    "    \n",
    "    #Calculate q1\n",
    "    delta_x2 = (p2[0] - p1[0]) * t\n",
    "    delta_y2 = (p2[1] - p1[1]) * t\n",
    "    q1_x = p1[0] + delta_x1\n",
    "    q1_y = p1[1] + delta_y1    \n",
    "    \n",
    "    #Calculate b(t)\n",
    "    delta_xt = (q1_x - q0_x) * t\n",
    "    delta_yt = (q1_y - q0_y) * t\n",
    "    x = q0_x + delta_xt\n",
    "    y = q0_y + delta_yt'''\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b38ae063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_kernel(ds, x, y, rows, cols, transform, xsize, ysize, databand=1, nodata=255):\n",
    "    ''' \n",
    "    Return an array of pixel values from ds centered around [x,y] of size (rows,cols)\n",
    "    '''\n",
    "    xoffset = int((x - transform[0])/transform[1]) - cols/2\n",
    "    yoffset = int((y - transform[3])/transform[5]) - rows/2  \n",
    "    databand = int(databand)\n",
    "    \n",
    "    # import pdb; pdb.set_trace()\n",
    "    k_inds, d_inds = get_offset_array_indices((rows, cols), (ysize, xsize), (yoffset, xoffset))\n",
    "\n",
    "    #import pdb; pdb.set_trace()\n",
    "    # If the lr of the kernel is to to the right or below the lower right pixel\n",
    "    if xoffset < 0 or yoffset < 0:\n",
    "        ds_cols = cols\n",
    "        ds_rows = rows    \n",
    "      \n",
    "        # Adjust the rows and columns to extract from the data to get only overlapping pixels \n",
    "        if xoffset < 0: \n",
    "            ds_cols = cols + xoffset\n",
    "            ''' return all nodata if outside any of ds bounds'''\n",
    "            xoffset = 0\n",
    "        if yoffset < 0: \n",
    "            ds_rows = rows + yoffset\n",
    "            yoffset = 0\n",
    "        \n",
    "        # Read the data as an array\n",
    "        #ds_ar = ds.ReadAsArray(xoffset, yoffset, ds_cols, ds_rows)\n",
    "        band = ds.GetRasterBand(databand)\n",
    "        try:\n",
    "            ds_ar = band.ReadAsArray(xoffset, yoffset, ds_cols, ds_rows)\n",
    "        except:\n",
    "            return np.full((rows, cols), nodata, dtype=np.int)\n",
    "        # Fill an array of 0s with the extracted array\n",
    "        ar = np.full((rows, cols), nodata, dtype=np.int)\n",
    "        ar[rows - ds_rows:, cols - ds_cols:] = ds_ar\n",
    "    \n",
    "        return ar\n",
    "    \n",
    "    # If the upper left of the kernel is left or above the ul pixel, respectively\n",
    "    elif xoffset + cols > xsize or yoffset + rows > ysize:\n",
    "        ds_cols = cols\n",
    "        ds_rows = rows\n",
    "        if xoffset + cols > xsize:\n",
    "            ds_cols = xsize - xoffset\n",
    "        if yoffset + rows >  ysize:\n",
    "            ds_rows = ysize - yoffset\n",
    "         \n",
    "          # Do the same as above\n",
    "        #ds_ar = ds.ReadAsArray(xoffset, yoffset, ds_cols, ds_rows)\n",
    "        band = ds.GetRasterBand(databand)\n",
    "        ds_ar = band.ReadAsArray(xoffset, yoffset, ds_cols, ds_rows)\n",
    "    \n",
    "        ar = np.zeros((rows, cols), dtype=np.int)\n",
    "        ar[:ds_rows, :ds_cols] = ds_ar\n",
    "        \n",
    "        return ar\n",
    "    \n",
    "    else: \n",
    "        ar = ds.GetRasterBand(databand).ReadAsArray(xoffset, yoffset, cols, rows) \n",
    "    \n",
    "    return ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71a437ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_color(in_array, color_dict):\n",
    "    ''' returns array where each value of in_array is replaced with an r, g, or b values '''\n",
    "    \n",
    "    max_val = np.max(in_array)\n",
    "    # If the entire window is outside the range of the dataset, return 0s\n",
    "    #if max_val == 0:\n",
    "        #return np.zeros(in_array.shape, dtype=int)\n",
    "        \n",
    "    mp = np.arange(0, max_val + 1, dtype=np.float64)\n",
    "    keys = [k for k in sorted(color_dict.keys()) if k in mp]\n",
    "    mp[keys] = [color_dict[k] for k in keys]\n",
    "    a_color = mp[in_array]\n",
    "\n",
    "    return a_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89e2e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_color(in_array, color_dict):\n",
    "    ''' \n",
    "    Return an array of interpolated color values based on data and color\n",
    "    values from color_dict\n",
    "    '''\n",
    "    #If the data values are negative values, bump the array and the keys up by the\n",
    "    #   minimum key value    \n",
    "\n",
    "    #warnings.filterwarnings('error')\n",
    "    keys = sorted(color_dict.keys())\n",
    "\n",
    "    # Get an array of indices from keys where each cell value of in_array \n",
    "    #   is replaced with the index of the next greatest value in keys\n",
    "    shape = in_array.shape\n",
    "    a_upper = np.searchsorted(keys, \n",
    "                              np.ravel(in_array),\n",
    "                              side='right').reshape(shape)\n",
    "    # Get the next lowest index value\n",
    "    a_lower = np.where(a_upper - 1 > 0, a_upper - 1, 0)\n",
    "    \n",
    "    # If any value > highest value in keys, set it to the highest index\n",
    "    a_upper[a_upper >= len(keys)] = len(keys) - 1\n",
    "    \n",
    "    # For each key, replace the key with the color value\n",
    "    #a_upval = a_upper.astype(np.float32)    \n",
    "    ind_to_color_dict = {i: color_dict[k] for i, k in enumerate(keys)}\n",
    "    ind_to_data_dict = {i:k for i, k in enumerate(keys)}\n",
    "    upper_rgb = replace_with_color(a_upper, ind_to_color_dict)\n",
    "    lower_rgb = replace_with_color(a_lower, ind_to_color_dict)\n",
    "    upper_val = replace_with_color(a_upper, ind_to_data_dict)\n",
    "    lower_val = replace_with_color(a_lower, ind_to_data_dict)\n",
    "    #import pdb; pdb.set_trace()\n",
    "\n",
    "    # Take off the top and the bottom\n",
    "    in_array[in_array > keys[-1]] = keys[-1]\n",
    "    in_array[in_array < keys[0]] = keys[0]\n",
    "    \n",
    "    # Interpolate\n",
    "    # Calc percent between stops and handle cases where there may be 0's\n",
    "    #   in the denominator (likely only where in_array was >= max key)\n",
    "    a_percent = np.full(shape, color_dict[keys[-1]], dtype=np.float64)\n",
    "    dist_from_lower = in_array - lower_val\n",
    "    stop_range = upper_val - lower_val\n",
    "    mask = stop_range == 0 # Assumes that wherever there's a zero in dedom, val should be max\n",
    "    a_percent[~mask] = np.true_divide(dist_from_lower, stop_range)[~mask]\n",
    "    a_out = upper_rgb * (a_percent) + lower_rgb * (1 - a_percent) \n",
    "    #import pdb; pdb.set_trace()\n",
    "    \n",
    "    return a_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "322597af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch_color(in_array, color_dict):\n",
    "    ''' '''\n",
    "\n",
    "    keys = sorted(color_dict.keys())\n",
    "    values = [color_dict[k] for k in keys]\n",
    "    \n",
    "    valrange = float(keys[1]-keys[0])\n",
    "    rgbrange = float(values[1]-values[0])\n",
    "    \n",
    "    in_array -= keys[0]\n",
    "\n",
    "    in_array[in_array < 0] = 0\n",
    "    \n",
    "    #figure out where each pixel is in the range\n",
    "    \n",
    "    in_array = np.true_divide(in_array, valrange)\n",
    "\n",
    "    in_array *= rgbrange\n",
    "    \n",
    "    return in_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88393257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_color(in_array, color_dict):\n",
    "    \n",
    "    keys = sorted(color_dict.keys())\n",
    "\n",
    "    # Get an array of indices from keys where each cell value of in_array \n",
    "    #   is replaced with the index of the next greatest value in keys\n",
    "    indices = np.searchsorted(keys, \n",
    "                              np.ravel(in_array),\n",
    "                              side='left').reshape(in_array.shape)\n",
    "    \n",
    "    mp = np.arange(0, max(keys) + 1)\n",
    "    #keys = [k for k in sorted(color_dict.keys()) if k in mp]\n",
    "    mp[keys] = [color_dict[k] for k in keys]\n",
    "    color_array = mp[indices]\n",
    "    \n",
    "    return color_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e948e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rgb_array(array, color_dicts, replace_method='d'):\n",
    "    ''' Return a 3D array using the specified replacement method. Array can\n",
    "        either be a 2D array or a dictionary of 2D arrays with keys r,g,b'''\n",
    "    \n",
    "    d = {}\n",
    "    # If array is a dictionary of arrays\n",
    "    if len(array) == 3:\n",
    "        for c in color_dicts:\n",
    "            ar = array[c]\n",
    "            ar[ar == -31999] = 0 # Replace nodata with 0\n",
    "            low = np.min(ar) - 1 # Get min data value\n",
    "            ar -= low\n",
    "            d_temp = {}\n",
    "            for k in color_dicts[c]: d_temp[k - low] = color_dicts[c][k]\n",
    "            d[c] = stretch_color(ar, d_temp)\n",
    "    \n",
    "    else:\n",
    "        for c in color_dicts:    \n",
    "            # Replace NoData value with 0\n",
    "            #array[array == -9999] = 0\n",
    "            \n",
    "            if replace_method == 's':\n",
    "                ar = replace_with_color(array, color_dicts[c])\n",
    "            \n",
    "            if replace_method == 'i':\n",
    "                ar = interpolate_color(array, color_dicts[c])\n",
    "            \n",
    "            if replace_method == 'c':\n",
    "                ar = classify_color(array, color_dicts[c])\n",
    "            \n",
    "            d[c] = ar\n",
    "    \n",
    "    a_bgr = np.dstack((d['b'], d['g'], d['r']))\n",
    "    \n",
    "    return a_bgr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ef7c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_color_bar(rgb_dict, bar_size):\n",
    "\n",
    "    length, width = bar_size\n",
    "    legend_dict = {}\n",
    "    \n",
    "    # Get the location along the lenght of the bar for each color stop\n",
    "    keys = np.sort(rgb_dict['r'].keys())\n",
    "    key_range = keys[-1] - keys[0]\n",
    "    position = (keys/float(key_range) * length).astype(int)\n",
    "    \n",
    "    for c in rgb_dict.keys():\n",
    "        this_dict = rgb_dict[c]\n",
    "        # Make a new dict where the position is the key instead of data val\n",
    "        pos_dict = {p: this_dict[k] for p, k in zip(position, keys)}\n",
    "        # Make array of position indices that's the width of the bar. Each\n",
    "        #   coloumn is identical.\n",
    "        pos_array = np.repeat(np.arange(length).reshape(length, 1), width, axis=1)\n",
    "        # Interpolate between each color stop\n",
    "        bar = interpolate_color(pos_array, pos_dict)\n",
    "        legend_dict[c] = bar[::-1] # reverse so max is on top\n",
    "        \n",
    "    color_bar = np.dstack((legend_dict['b'], legend_dict['g'], legend_dict['r']))\n",
    "    \n",
    "    return color_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b21dc4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_legend(legend_shape, title, values, color_mode, rgb_dict, font_path, font_size=None):\n",
    "    ''' values is data_val: label series'''    \n",
    "    \n",
    "    legend_h, legend_w = legend_shape\n",
    "\n",
    "    if font_size is None:\n",
    "        label_font_size = int(legend_h * .03 * 1.467)\n",
    "    else:\n",
    "        label_font_size = int(font_size * 1.467)#*1.46 makes array size == font size\n",
    "    title_font_size = int(label_font_size * 1.67)\n",
    "    label_pad_w = int(label_font_size/1.467)\n",
    "    min_canvas_pad_w = int(label_pad_w * 1.5)\n",
    "    \n",
    "    # Draw text\n",
    "    labels = {}\n",
    "    label_shapes = []\n",
    "    keys = values.sort_index().values\n",
    "    for string in keys:\n",
    "       text_array, _ = draw_text(string, font_path, label_font_size, bg_color=(0,0,0), txt_color=(255,255,255))\n",
    "       text_array = clip_nodata_edges(text_array, 0)\n",
    "       text_shape = text_array.shape[:2]\n",
    "       label_shapes.append(text_shape)\n",
    "       labels[string] = text_array\n",
    "    label_heights, label_lengths = zip(*label_shapes)\n",
    "    max_label_w = max(label_lengths)\n",
    "    max_label_h = max(label_heights)\n",
    "    \n",
    "    title_array, _ = draw_text(title, font_path, title_font_size, bg_color=(0,0,0), txt_color=(255,255,255))\n",
    "    title_array = clip_nodata_edges(title_array, 255)\n",
    "    title_h, title_w = title_array.shape[:2]\n",
    "    title_pad = title_h * 1.5\n",
    "    min_canvas_offset = int(title_h + title_pad)\n",
    "    max_canvas_h = legend_h - min_canvas_offset - title_pad#this title pad == bottom pad\n",
    "    \n",
    "    if color_mode == 'i':\n",
    "        \n",
    "        bar_h = min(int(legend_h * 0.66), int(legend_w/2))\n",
    "        bar_w = int(bar_h * 0.1)\n",
    "        color_bar = make_color_bar(rgb_dict, (bar_h, bar_w))\n",
    "        bar_h, bar_w, _ = color_bar.shape # In case it's changed by a pixel\n",
    "        \n",
    "        legend_w = max(legend_w, bar_w + label_pad_w * 3 + max_label_w, title_w)\n",
    "        canvas_h = bar_h + min_canvas_offset        \n",
    "        bar_uy = legend_h/2 - canvas_h/2 + min_canvas_offset\n",
    "        bar_ly = bar_uy + bar_h\n",
    "        bar_lx = legend_w/2 - (bar_w + label_pad_w + max_label_w)/2\n",
    "        bar_rx = bar_lx + bar_w\n",
    "        \n",
    "        legend = np.full((legend_h, legend_w, 3), 255, dtype=np.uint8)\n",
    "        alpha = np.full((legend_h, legend_w, 3), 0, dtype=np.uint8)\n",
    "        legend[bar_uy : bar_ly, bar_lx : bar_rx] = color_bar\n",
    "        alpha[bar_uy : bar_ly, bar_lx : bar_rx] = 255\n",
    "        \n",
    "        max_str = values.iloc[-1]\n",
    "        min_str = values.iloc[0]\n",
    "        label_lx = bar_rx + label_pad_w\n",
    "        alpha[bar_uy : bar_uy + label_heights[-1], label_lx : label_lx + label_lengths[-1]] = labels[max_str]\n",
    "        alpha[bar_ly - label_heights[0]: bar_ly, label_lx : label_lx + label_lengths[0]] = labels[min_str]\n",
    "        \n",
    "        title_ur = bar_uy - min_canvas_offset\n",
    "        title_lc = legend_w/2 - title_w/2\n",
    "        title_lr = title_ur + title_h\n",
    "        title_rc = title_lc + title_w\n",
    "        alpha[title_ur : title_lr, title_lc : title_rc] = title_array\n",
    "    \n",
    "    elif color_mode == 's':\n",
    "        rgb = pd.DataFrame(rgb_dict)[['b','g','r']]\n",
    "        patch_h = max(max_label_h, min(int(legend_h * 0.06), int(legend_w * 0.05)))\n",
    "        patch_w = int(patch_h * 1.5)\n",
    "        patches = {}\n",
    "        for k, v in values.iteritems():\n",
    "            patches[v] = np.tile(rgb.loc[k], patch_h * patch_w).reshape(patch_h, patch_w, 3)\n",
    "        n_patches = len(keys)\n",
    "        all_patches_h = patch_h * n_patches\n",
    "        pad_h = max(patch_h/2,\n",
    "                    min(patch_h/2, (max_canvas_h - all_patches_h)/(n_patches - 1))\n",
    "                    )\n",
    "\n",
    "        column_h = all_patches_h + pad_h * (n_patches - 1)\n",
    "        used_h = column_h + min_canvas_offset # Used height of whole legend\n",
    "        canvas_row_off = max(min_canvas_offset, legend_h/2 - used_h/2)\n",
    "        \n",
    "        if column_h > max_canvas_h or pad_h < 0:\n",
    "            # Split into 2 columns\n",
    "            half_patches = int(round(n_patches/2.))\n",
    "            left_col_w = max(label_lengths[:half_patches]) + label_pad_w + patch_w\n",
    "            right_col_w = max(label_lengths[half_patches:]) + label_pad_w + patch_w\n",
    "            column_pad = int(label_pad_w * 1.3)\n",
    "            canvas_w =  left_col_w + right_col_w + column_pad\n",
    "            legend_w = max(legend_w, canvas_w + min_canvas_pad_w * 2)\n",
    "            canvas_col_off = legend_w/2 - canvas_w/2\n",
    "            pad_h = patch_h/2\n",
    "            \n",
    "            column_h = patch_h * half_patches + pad_h * (half_patches - 1)\n",
    "            used_h = column_h + min_canvas_offset\n",
    "            canvas_row_off = max(min_canvas_offset,\n",
    "                                 legend_h/2 - used_h/2 + min_canvas_offset/2)\n",
    "            if column_h > max_canvas_h:\n",
    "                raise ValueError('legend exceeds allowable size')\n",
    "            left_col_t = np.arange(half_patches) * (pad_h + patch_h) + canvas_row_off\n",
    "            left_col_l = np.full(half_patches, canvas_col_off, dtype=np.int16)\n",
    "            patch_tops = np.concatenate([left_col_t, left_col_t[:n_patches/2]]).astype(int)\n",
    "            patch_lefts = np.concatenate([left_col_l, left_col_l[:n_patches/2] + left_col_w + column_pad])\n",
    "            patch_bottoms = patch_tops + patch_h\n",
    "            patch_rights = patch_lefts + patch_w\n",
    "        \n",
    "        else: # All one column\n",
    "            column_w = (patch_w + label_pad_w + max_label_w)\n",
    "            canvas_w = max(column_w, legend_w - min_canvas_pad_w * 2)\n",
    "            legend_w = max(legend_w, canvas_w + min_canvas_pad_w * 2)\n",
    "            canvas_col_off = legend_w/2 - canvas_w/2\n",
    "\n",
    "            patch_tops = (np.arange(n_patches) * (pad_h + patch_h) + canvas_row_off).astype(int)\n",
    "            patch_bottoms = patch_tops + patch_h\n",
    "            patch_lefts = np.full(n_patches, canvas_col_off, dtype=np.int16)\n",
    "            patch_rights = patch_lefts + patch_w\n",
    "            \n",
    "        patch_inds = pd.DataFrame({'t': patch_tops,\n",
    "                                   'b': patch_bottoms,\n",
    "                                   'l': patch_lefts,\n",
    "                                   'r': patch_rights,\n",
    "                                   'label_h': label_heights,\n",
    "                                   'label_w': label_lengths},\n",
    "                                   index=keys)\n",
    "        \n",
    "        legend = np.full((legend_h, legend_w, 3), 255, dtype=np.uint8)\n",
    "        alpha = np.full((legend_h, legend_w, 3), 0, dtype=np.uint8)\n",
    "        for k, inds in patch_inds.iterrows():\n",
    "            legend[inds.t : inds.b, inds.l : inds.r] = patches[k]\n",
    "            alpha[inds.t : inds.b, inds.l : inds.r] = 255\n",
    "            label_ur = patch_h/2 - inds.label_h/2 + inds.t\n",
    "            label_lc = inds.r + label_pad_w\n",
    "            label_lr = label_ur + inds.label_h\n",
    "            label_rc = label_lc + inds.label_w\n",
    "            # Only add label to alpha because legend is all white except patches\n",
    "            alpha[label_ur : label_lr, label_lc : label_rc] = labels[k]\n",
    "    \n",
    "        title_ur = patch_tops.min() - min_canvas_offset\n",
    "        title_lc = canvas_col_off + canvas_w/2 - title_w/2\n",
    "        title_lr = title_ur + title_h\n",
    "        title_rc = title_lc + title_w\n",
    "        alpha[title_ur : title_lr, title_lc : title_rc] = title_array\n",
    "    \n",
    "    return legend, alpha/255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91e776f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_halo(array, txt_size, ul_y, opacity=.4): \n",
    "    ''' calculates in place rgb values of a transparent rectangle in array to fit txt_size '''\n",
    "    \n",
    "    rows, cols, bands = array.shape\n",
    "    halo_buff = int(txt_size[0]/1.5)\n",
    "    halo_rows = txt_size[0] + halo_buff\n",
    "    halo_cols = txt_size[1] + halo_buff\n",
    "    \n",
    "    x_off = int(cols/2 - halo_cols/2)\n",
    "    y_off = int(ul_y - halo_buff/2)\n",
    "    \n",
    "    array = array.astype(float)\n",
    "\n",
    "    #array[y_off : y_off + halo_rows, x_off : x_off + halo_cols] *= (1 - opacity)\n",
    "    #array[y_off : y_off + halo_rows, x_off : x_off + halo_cols] += int(0 * opacity)\n",
    "    \n",
    "    # import pdb; pdb.set_trace()\n",
    "    halo = filters.gaussian(array[y_off : y_off + halo_rows, x_off : x_off + halo_cols], 3, multichannel=True)\n",
    "    \n",
    "    halo_alpha = np.zeros((halo_rows, halo_cols))\n",
    "    halo_alpha[3:-3, 3:-3] = 1\n",
    "    halo_alpha = filters.gaussian(halo_alpha, 2)\n",
    "    halo = blend_arrays(array[y_off:y_off + halo_rows, x_off:x_off + halo_cols], halo, halo_alpha)\n",
    "    \n",
    "    halo_alpha = halo_alpha - (1 - opacity)\n",
    "    halo_alpha[halo_alpha < 0] = 0\n",
    "    halo = blend_arrays(halo, halo_alpha , halo_alpha)\n",
    "    \n",
    "    array[y_off:y_off + halo_rows, x_off:x_off + halo_cols] = halo\n",
    "    \n",
    "    \n",
    "    return array.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e945244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shp_params(shp):\n",
    "    # Open the data source and read in the extent\n",
    "    src_ds = ogr.Open(shp)\n",
    "    src_lyr = src_ds.GetLayer()\n",
    "    x_min, x_max, y_min, y_max = src_lyr.GetExtent()\n",
    "    srs = src_lyr.GetSpatialRef()     \n",
    "    \n",
    "    # Create the destination data source\n",
    "    x_size = int((x_max - x_min) / res)\n",
    "    y_size = int((y_max - y_min) / res)\n",
    "    \n",
    "    src_ds = None\n",
    "    src_lyr = None   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d1be208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prj(prj):\n",
    "    \n",
    "    with open(prj) as txt:\n",
    "        params = [line for line in txt][0].split(',PARAMETER')\n",
    "    \n",
    "    for p in params:    \n",
    "        if '\"latitude_of_origin\"' in p:\n",
    "            origin = float(p.split(',')[1].replace(']',''))\n",
    "        if '\"central_meridian\"' in p:\n",
    "            meridian = float(p.split(',')[1].replace(']',''))\n",
    "        if '\"standard_parallel_1\"' in p:\n",
    "            strd_par1 = float(p.split(',')[1].replace(']',''))\n",
    "        if '\"standard_parallel_2\"' in p:\n",
    "            strd_par2 = float(p.split(',')[1].replace(']',''))    \n",
    "\n",
    "    return origin, meridian, strd_par1, strd_par2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc48cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_array(ar, rotation, fill_val, clip=False, buffer_size=None):\n",
    "    ''' Rotate an array, adjusting it's shape as necessary to keep all data\n",
    "    Could also do this with skimage.tranform.rotate() '''\n",
    "    # Get angles of upper right and upper left corners\n",
    "    n_rows = ar.shape[0]\n",
    "    n_cols = ar.shape[1]\n",
    "    diag_distance = (n_rows ** 2 + n_cols ** 2) ** 0.5\n",
    "    angle_ur0 = math.degrees(math.atan(n_rows/float(n_cols)))\n",
    "    angle_ul0 = math.degrees(math.atan(n_rows/-float(n_cols)))\n",
    "    \n",
    "    # Add rotation\n",
    "    angle_ur1 = math.radians(angle_ur0 + rotation)\n",
    "    angle_ul1 = math.radians(angle_ul0 + rotation)\n",
    "    \n",
    "    # Get sin and cos of top and right corners. Need to use both angles because\n",
    "    #   depending on rotation, one might stick out more above or to the side than the other\n",
    "    rotated_rows = int(max(abs(math.sin(angle_ur1)), abs(math.sin(angle_ul1))) * diag_distance) \n",
    "    rotated_cols = int(max(abs(math.cos(angle_ur1)), abs(math.cos(angle_ul1))) * diag_distance)\n",
    "    \n",
    "    # Make array with shape of (max(rows, sin * diag), max(cols, cos * diag))\n",
    "    buf_rows = max(n_rows, rotated_rows)\n",
    "    buf_cols = max(n_cols, rotated_cols)\n",
    "    buf_shape = [buf_rows, buf_cols] + [dim for dim in ar.shape[2:]]\n",
    "    dtype = ar.dtype\n",
    "    ar_buf = np.full(buf_shape, fill_val, dtype=dtype)\n",
    "    \n",
    "    # Place ar in center of buffered array\n",
    "    row_dif = (buf_rows - n_rows) / 2\n",
    "    col_dif = (buf_cols - n_cols) / 2\n",
    "    ar_buf[row_dif : row_dif + n_rows, col_dif : col_dif + n_cols] = ar\n",
    "    \n",
    "    # Rotate\n",
    "    ar_rot = transform.rotate(ar_buf, rotation, preserve_range=True)\n",
    "    \n",
    "    # clip edges of nodata\n",
    "    if clip:\n",
    "        _, inds = clip_nodata_edges(ar_rot, fill_val, return_inds=True, buffer_size=buffer_size)\n",
    "        return ar_rot, inds\n",
    "        \n",
    "    return ar_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c28641bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_extent_pts(ds_extent, base_ar, pt_size, pt_mask, df, pt_time):\n",
    "    '''\n",
    "    Return an array with points from df plotted with alpha values from df\n",
    "    '''\n",
    "    rgb = np.array([0, 0, 255])\n",
    "    \n",
    "    # For each point, blend colors with color1 * (1 - alpha) + color2 * alpha\n",
    "    this_ar = np.copy(base_ar)\n",
    "    #print base_ar\n",
    "    for ind, row in df.iterrows():\n",
    "        t = row['alpha']/pt_time\n",
    "        y_off = int(np.round(row['y_off'], 0)) # Can't calculate upstream because df contains nans)\n",
    "        x_off = int(np.round(row['x_off'], 0))\n",
    "    \n",
    "        #alpha = -(t-1)**3\n",
    "        alpha = np.round(1 / (16 * (t + .25)**2), 2)\n",
    "        if t > 1: alpha=0\n",
    "        a_inds, m_inds = get_offset_array_indices(this_ar[y_off : y_off + pt_size, x_off : x_off + pt_size].shape[:-1], pt_mask.shape, (0,0))\n",
    "        #print m_inds\n",
    "        this_mask = np.copy(pt_mask[m_inds[0]:m_inds[1], m_inds[2]:m_inds[3]])\n",
    "        pt_row_lr = y_off + pt_size # lower right row\n",
    "        pt_col_lr = x_off + pt_size # lower right col\n",
    "        ar_pt = this_ar[y_off : pt_row_lr, x_off : pt_col_lr]\n",
    "        ar_pt[this_mask] *= (1 - alpha)    \n",
    "        ar_pt[this_mask] += (rgb * alpha)\n",
    "        \n",
    "    return this_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1cca682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur_background(ar, blur, buffer_inds, bg_opacity=None):\n",
    "    ''' buffer inds define distance of edge transition '''\n",
    "    blurred = filters.gaussian(ar.astype(float), sigma=blur, multichannel=True).astype(np.uint8)\n",
    "    bg_alpha = np.zeros(ar.shape[:2]) # Alpha array (0== no original array)\n",
    "    bg_alpha[buffer_inds[0]:buffer_inds[1], buffer_inds[2]:buffer_inds[3]] = 1\n",
    "    bg_alpha = filters.gaussian(bg_alpha, 3) # blur edges of alpha for transition\n",
    "    ar = blend_arrays(ar, blurred, bg_alpha) # blend arrays\n",
    "\n",
    "    # Make it darker\n",
    "    ''' make it possible to lighten, not just darken'''\n",
    "    if bg_opacity:\n",
    "        bg_alpha = bg_alpha - (1 - bg_opacity)\n",
    "        bg_alpha[bg_alpha < 0] = 0\n",
    "    \n",
    "    return blend_arrays(ar, bg_alpha, bg_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eea0b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlay_inds(position, ext_size, array_size, bg_buffer=None, ctr_coords=None):\n",
    "    \n",
    "    ext_rows, ext_cols = ext_size\n",
    "    if bg_buffer is None:\n",
    "        bg_buffer = min(ext_rows, ext_cols)/10\n",
    "    bg_size =  ext_rows + bg_buffer, ext_cols + bg_buffer\n",
    "    n_rows, n_cols = array_size\n",
    "    \n",
    "    try:\n",
    "        position = position.lower()\n",
    "    except SyntaxError:\n",
    "        raise ValueError('position must be type str, received %s' % type(position))\n",
    "    \n",
    "    if not position:\n",
    "        position = 'lower right'\n",
    "    if 'lower' in position: \n",
    "        rgb_r_inds = n_rows - bg_size[0], n_rows # Inds for full extent including buffer\n",
    "        ext_r_inds = n_rows - ext_rows, n_rows #inds of just extent\n",
    "        buf_r_inds = bg_buffer, bg_size[0]\n",
    "    else: \n",
    "        rgb_r_inds = 0, bg_size[0]\n",
    "        ext_r_inds = 0, ext_rows\n",
    "        buf_r_inds = 0, ext_rows\n",
    "    if 'right' in position: \n",
    "        rgb_c_inds = n_cols - bg_size[1], n_cols\n",
    "        ext_c_inds = n_cols - ext_cols, n_cols\n",
    "        buf_c_inds = bg_buffer, bg_size[1]\n",
    "    elif 'left' in position:\n",
    "        rgb_c_inds = 0, bg_size[1]\n",
    "        ext_c_inds = 0, ext_cols\n",
    "        buf_c_inds = ext_cols, bg_size[1]\n",
    "    elif 'center' in position:\n",
    "        if ctr_coords is None:\n",
    "            ctr_coords = [0, n_cols]\n",
    "        ctr_c = ctr_coords[0] + (ctr_coords[1] - ctr_coords[0])/2\n",
    "        rgb_c_inds = ctr_c - ext_cols/2, ctr_c - ext_cols/2 + ext_cols\n",
    "        ext_c_inds = rgb_c_inds # no buffer\n",
    "        buf_c_inds = rgb_c_inds\n",
    "    \n",
    "    rgb_inds = [rgb_r_inds[0], rgb_r_inds[1], rgb_c_inds[0], rgb_c_inds[1]]\n",
    "    ovr_inds = [ext_r_inds[0], ext_r_inds[1], ext_c_inds[0], ext_c_inds[1]]\n",
    "    buf_inds = [buf_r_inds[0], buf_r_inds[1], buf_c_inds[0], buf_c_inds[1]]\n",
    "    \n",
    "    return rgb_inds, ovr_inds, buf_inds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed60538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_extent_to_frame(rgb_array, extent_ar, alpha_ar, position, bg_opacity=.4, blur=5, rotation=None, legend_width=None, ctr_coords=None):\n",
    "    ''' '''\n",
    "    # Have to rotate here because adding extent points won't work. Consider\n",
    "    #   trying to transform the coords too so I just rotate once\n",
    "    if rotation:\n",
    "        extent_ar = rotate_array(extent_ar, rotation, 0)#, True, 10)\n",
    "        _, clip_inds = clip_nodata_edges(extent_ar, 0, buffer_size=.1, return_inds=True)\n",
    "        alpha_ar = rotate_array(alpha_ar, rotation, 0)\n",
    "        first_row, last_row, first_col, last_col = clip_inds\n",
    "        extent_ar = extent_ar[first_row : last_row, first_col : last_col]\n",
    "        alpha_ar = alpha_ar[first_row : last_row, first_col : last_col]\n",
    "    \n",
    "    ext_rows, ext_cols, _ = extent_ar.shape \n",
    "    if legend_width > ext_cols:\n",
    "        # Make extent ar the same width as legend and center it\n",
    "        new_extent = np.full((ext_rows, legend_width, 3), 0, dtype=np.uint8)\n",
    "        ul_c = legend_width/2 - ext_cols/2\n",
    "        new_extent[:, ul_c : ul_c + ext_cols] = extent_ar\n",
    "        extent_ar = new_extent\n",
    "    \n",
    "    rgb_inds, ext_inds, buf_inds = get_overlay_inds(position, (ext_rows, ext_cols), rgb_array.shape[:2], ctr_coords=ctr_coords)\n",
    "    \n",
    "\n",
    "    # Make an array of the background that's blurred\n",
    "    if blur: #might already be blurred if there's a legend\n",
    "        ar = rgb_array[rgb_inds[0]:rgb_inds[1], rgb_inds[2]:rgb_inds[3]] \n",
    "        blurred = blur_background(ar, blur, buf_inds, bg_opacity=bg_opacity)\n",
    "        # replace the footprint of the extent indicator with the darkerned, blurred backgorund\n",
    "        rgb_array[rgb_inds[0]:rgb_inds[1], rgb_inds[2]:rgb_inds[3]] = blurred\n",
    "    # Now use only the extent part of the extent array on top of the blurred background\n",
    "    #   alpha_ar is an array that describes which parts of extent_ar should be used (i.e.,\n",
    "    #   just the interior parts of the poloygons)\n",
    "    final_extent = blend_arrays(rgb_array[ext_inds[0]:ext_inds[1], ext_inds[2]:ext_inds[3]], extent_ar, opacity=alpha_ar)\n",
    "    rgb_array[ext_inds[0]:ext_inds[1], ext_inds[2]:ext_inds[3]] = final_extent\n",
    "    \n",
    "    return rgb_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7f7129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_arrays(ar1, ar2, opacity=.15):\n",
    "    ''' Blend ar1 with ar2 with opacity of ar2'''\n",
    "    if len(ar1.shape) == 3 and len(ar2.shape) == 2:\n",
    "        ar2 = np.dstack([ar2 for i in range(ar1.shape[2])])\n",
    "    \n",
    "    if type(opacity) != float and len(opacity.shape) == 2:\n",
    "        opacity = np.dstack([opacity for i in range(ar1.shape[2])])\n",
    "\n",
    "    ar2_b = ar2 * opacity\n",
    "    ar1_b = ar1 * (1 - opacity)\n",
    "    blended = ar2_b + ar1_b\n",
    "        \n",
    "    return blended\n",
    "\n",
    "\n",
    "def blend_multiply(ar1, ar2):\n",
    "    \n",
    "    if len(ar1.shape) == 3 and len(ar2.shape) == 2:\n",
    "        ar2 = np.dstack([ar2 for i in range(ar1.shape[2])])\n",
    "    \n",
    "    blended = np.round((ar1 * ar2)/255.0, 0).astype(np.int)\n",
    "    \n",
    "    return blended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b6980a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offset_array_indices(ar1_shape, ar2_shape, ar2_offset):\n",
    "    '''\n",
    "    Return the indices to broadcast ar2 into ar1\n",
    "    '''\n",
    "    # Initialize stuff\n",
    "    yoffset, xoffset = ar2_offset\n",
    "    ar2_rows, ar2_cols = ar2_shape\n",
    "    ar1_rows, ar1_cols = ar1_shape\n",
    "    # Initialize indices for arrays in case they don't need to be set by\n",
    "    #  the offset\n",
    "    ar2_col_l, ar2_row_u, ar1_col_l, ar1_row_u = 0, 0, 0, 0\n",
    "    ar2_col_r, ar2_row_d = ar2_cols, ar2_rows\n",
    "    ar1_col_r, ar1_row_d = ar1_cols, ar1_rows\n",
    "    \n",
    "    # If the upper left pixel of the data array is left of the ul pixel \n",
    "    #   of ar1, reset the index of the ar1 to get only cols to the right\n",
    "    #   of the offset\n",
    "    if xoffset < 0:\n",
    "        ar2_col_l = abs(xoffset)\n",
    "    # Otherwise, do the same for ar1\n",
    "    else:\n",
    "        ar1_col_l = xoffset\n",
    "        \n",
    "    # Do the same checks and adjustments if the ar2 ul is above the ar1 ul\n",
    "    if yoffset < 0:\n",
    "        ar2_row_u = abs(yoffset)\n",
    "    else:\n",
    "        ar1_row_u = yoffset\n",
    "    \n",
    "    # If the lower right pixel of ar2 is to the right of the lr pixel of\n",
    "    #   ar1, truncate ar2 by the difference\n",
    "    if xoffset + ar2_cols > ar1_cols:\n",
    "        ar2_col_r = ar1_cols - xoffset\n",
    "    # Otherwise, the ar1 array needs to be truncated\n",
    "    else:\n",
    "        ar1_col_r = ar2_cols + xoffset\n",
    "\n",
    "    # Do the same if either lr is below the other\n",
    "    if yoffset + ar2_rows > ar1_rows:\n",
    "        ar2_row_d = ar1_rows - yoffset\n",
    "    # Otherwise, the ar1 array needs to be truncated\n",
    "    else:\n",
    "        ar1_row_d = ar2_rows + yoffset\n",
    "\n",
    "    ar1_bounds = ar1_row_u, ar1_row_d, ar1_col_l, ar1_col_r\n",
    "    ar2_bounds = ar2_row_u, ar2_row_d, ar2_col_l, ar2_col_r\n",
    "    \n",
    "    return ar1_bounds, ar2_bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "adeb8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def burn_shp_to_array(rgb_array, coords, bounds, x, y, res, out_dir, burn_rgb=[255, 255, 255], opacity=.5, style='glow', blur=50):\n",
    "    ''' Burn shp into rgb_array\n",
    "    -make image from shape with solid black fill on white bg\n",
    "    -blur it so that some white gets into the shape\n",
    "    -use inverse as alpha array\n",
    "    '''\n",
    "\n",
    "    x_min, x_max, y_min, y_max = bounds\n",
    "    delta_x = x_max - x_min\n",
    "    delta_y = y_max - y_min\n",
    "    x_size = int((delta_x / res))\n",
    "    y_size = int((delta_y / res))\n",
    "    if x_size > y_size: max_size = x_size\n",
    "    else: max_size = y_size\n",
    "    \n",
    "    ''' move this to main()'''\n",
    "    burn_img = os.path.join(out_dir, 'burn.npy') #add tag from keyword\n",
    "    if not os.path.exists(burn_img):\n",
    "        if style == 'glow':\n",
    "            _, ar_shp, _ = shp_to_array(coords, bounds, max_size, out_dir, out_file='burn.png', pad=0)\n",
    "            #os.remove(burn_img)\n",
    "            #ar_shp = 1 - ar_shp\n",
    "            #ar_alpha = 1 - filters.gaussian(ar_shp, 30, multichannel=True)\n",
    "            #ar_alpha[ar_shp == 1] = 1\n",
    "            ar_alpha = filters.gaussian(ar_shp, blur, multichannel=True)\n",
    "            ar_alpha[ar_shp == 0] = 1\n",
    "            np.save(burn_img, ar_alpha)\n",
    "        elif style == 'outline':\n",
    "            a = 1\n",
    "    else:\n",
    "        ar_alpha = np.load(burn_img)\n",
    "        \n",
    "    # Calc offset\n",
    "    shp_shape = ar_alpha.shape\n",
    "    img_rows, img_cols, bands = rgb_array.shape\n",
    "    img_x = x - (img_cols/2 * res)\n",
    "    img_y = y - (img_rows/2 * -res)\n",
    "    col_off = int((x_min - img_x)/res)# * x_ar_scale)# + 10\n",
    "    row_off = int((y_max - img_y)/-res)# * y_ar_scale)# + 207\n",
    "    r_inds, s_inds = get_offset_array_indices(rgb_array.shape[:-1], shp_shape[:-1], (row_off, col_off))\n",
    "    \n",
    "    # If burn_shp is out of the frame, return rgb_array\n",
    "    if np.any(np.array(r_inds) < 0) or np.any(np.array(s_inds) < 0):\n",
    "        return rgb_array\n",
    "    \n",
    "    rgb = rgb_array[r_inds[0]:r_inds[1], r_inds[2]:r_inds[3]]\n",
    "    alpha =  1 - ar_alpha[s_inds[0]:s_inds[1], s_inds[2]:s_inds[3]] #* opacity\n",
    "    burned = burn_rgb * alpha + rgb * (1 - alpha)\n",
    "    \n",
    "    rgb_array[r_inds[0]:r_inds[1], r_inds[2]:r_inds[3]] = burned\n",
    "    #import pdb; pdb.set_trace()\n",
    "    \n",
    "    return rgb_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8938619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_imgs(start_frame, basepath, nframes):\n",
    "    \n",
    "    tag_len = len(start_frame)\n",
    "    \n",
    "    start_img = basepath + '_%s.png' % start_frame\n",
    "    int_start_frame = int(start_frame)\n",
    "    end_frame = '000000%s' % (int_start_frame + nframes)\n",
    "    end_frame = end_frame[-tag_len:]\n",
    "    int_end_frame = int(end_frame)\n",
    "    end_img = start_img.replace(start_frame, end_frame)\n",
    "    \n",
    "    start_ar = cv2.imread(start_img)\n",
    "    end_ar = cv2.imread(end_img)\n",
    "    \n",
    "    out_img = basepath + '_%s.png'\n",
    "    for i in range(1, nframes): \n",
    "        percent = float(i)/nframes\n",
    "        this_ar = end_ar * percent + start_ar * (1 - percent)\n",
    "        frame_str = '000000%s' % (int_start_frame + i)\n",
    "        this_img = out_img % frame_str[-tag_len:]\n",
    "        cv2.imwrite(this_img, this_ar)                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bddb6cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pan_sharpen(ar_rgb, ar_pan, sharpen_type):\n",
    "    #Brovey pan sharpening:\n",
    "    #   Red_out = Red_in / [(blue_in + green_in + red_in) * Pan]\n",
    "    def brovey(ar_rgb, ar_pan):\n",
    "        rgb_sum = np.sum(ar_rgb, axis=2)\n",
    "        \n",
    "        for band in range(ar_rgb.shape[2]):\n",
    "            this_band = ar_rgb[..., band].astype(np.float64)\n",
    "            this_band = this_band / rgb_sum * ar_pan\n",
    "            #ar_rgb[:, :, band] = this_band / rgb_sum * ar_pan\n",
    "            import pdb; pdb.set_trace()\n",
    "        return ar_rgb\n",
    "    \n",
    "    def simple_mean(ar_rgb, ar_pan):\n",
    "        \n",
    "        for band in range(ar_rgb.shape[2]):\n",
    "            this_band = ar_rgb[..., band]\n",
    "            ar_rgb[..., band] = (this_band + ar_pan)/2.\n",
    "        \n",
    "        return ar_rgb\n",
    "    \n",
    "    functions = {'brovey': brovey,\n",
    "                 'simple_mean': simple_mean}\n",
    "    \n",
    "    return functions[sharpen_type](ar_rgb, ar_pan)\n",
    "\n",
    "\n",
    "\n",
    "def gamma_stretch(ar, gamma=1.0):\n",
    "    ''' O = I ^ (1 / G)'''\n",
    "    \n",
    "    stretched = (ar/255.) ** (1/gamma)\n",
    "    stretched = stretched/stretched.max() * 255\n",
    "    \n",
    "    return stretched.astype(np.uint8)\n",
    "\n",
    "\n",
    "\n",
    "def str_to_number(string):\n",
    "    \n",
    "    if '.' in string:\n",
    "        try: n = float(string)\n",
    "        except: n = None\n",
    "    else:\n",
    "        try: n = int(string)\n",
    "        except: n = None\n",
    "    \n",
    "    if n is None:\n",
    "        raise ValueError('numeric string not understood: %s' % string)\n",
    "    \n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da6e9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b49843a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(params, bands=None, extent_shp=None, extent_info=None, hillshade_path=None, hs_opacity=0, n_jobs=0, mosaic_path=None, extent_position ='lower right', legends=None, max_legend_w=None, legend_inds=None, label_font_size=None):\n",
    "    t0 = time.time()\n",
    "    # Read params and make variables from each line\n",
    "    inputs = read_params(params)\n",
    "    \n",
    "    # state the variables mannual as exec works differently from python 2 to 3\n",
    "    # for var in inputs:\n",
    "        # print(('{0} = {1}').format(var, inputs[var]))\n",
    "        # exec(('{0} = str(r{1})').format(var, inputs[var]), globals(), locals())\n",
    "    file_info = inputs['file_info']\n",
    "    xy_txt = inputs['xy_txt']\n",
    "    year_min = inputs['year_min']\n",
    "    year_max = inputs['year_max']\n",
    "    font_path = inputs['font_path']\n",
    "    rows = inputs['rows']\n",
    "    cols = inputs['cols']\n",
    "    fps = inputs['fps']\n",
    "    fade_time = inputs['fade_time']\n",
    "    bands = inputs['bands']\n",
    "    m_per_frame = inputs['m_per_frame']\n",
    "    rgb_sep = inputs['rgb_sep']\n",
    "    out_dir = inputs['out_dir']\n",
    "    # extent_shp = inputs['extent_shp']\n",
    "    pt_time = inputs['pt_time']\n",
    "    pt_frequ = inputs['pt_frequ']\n",
    "    max_extent_size = inputs['max_extent_size']\n",
    "    extent_position = inputs['extent_position']\n",
    "    extent_rotation = inputs['extent_rotation']\n",
    "    movie_basename = inputs['movie_basename']\n",
    "    # n_jobs = inputs['n_jobs']\n",
    "    \n",
    "    #out_dir = \"/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Output\"\n",
    "    \n",
    "    if not os.path.exists(out_dir): \n",
    "        os.mkdir(out_dir)\n",
    "\n",
    "    # Make ints out of all numeric params\n",
    "    try:\n",
    "        rows = int(rows)\n",
    "        cols = int(cols)\n",
    "        years = range(int(year_min), int(year_max) + 1)\n",
    "        m_per_frame = int(m_per_frame)\n",
    "        fps = int(fps)\n",
    "        fade_time = int(fade_time)\n",
    "        str_check = font_path, file_info, xy_txt, out_dir, rgb_sep\n",
    "    except NameError as e:\n",
    "        missing_var = str(e).split(\"'\")[1]\n",
    "        msg = \"Variable '%s' not specified in param file:\\n%s\" % (missing_var, params)\n",
    "        raise NameError(msg)\n",
    "\n",
    "    \n",
    "    # Get img paths, and make dictionary of dictionaries where \n",
    "    # dirs[search string] = {'dir': directory, 'replace': replacement method}\n",
    "    # Dictionary may not be necessary because once I find the files, other methods handle slecting the right img\n",
    "    imgs = []\n",
    "    file_info = read_txt(file_info)\n",
    "    rgb_dics = {}\n",
    "    for i, info in file_info.iterrows():\n",
    "        imgs.extend(search_directory(info.search_dir, info.search_str, info.file_extension))\n",
    "        this_rgb_path = info.rgb_path\n",
    "        this_rgb_dict = read_rgb(this_rgb_path)#, rgb_sep)\n",
    "        rgb_dics[info.search_str] = this_rgb_dict\n",
    "            \n",
    "        if not check_rgb(this_rgb_dict):\n",
    "            message = (('\\nCould not read RGB the text file for search string {0}:' +\\\n",
    "                      '\\n{1}\\nPlease check the file path and rgb_sep in master ' +\\\n",
    "                      'parameters file. If the RGB file is still not working, ' +\\\n",
    "                      'try creating one from scratch in a text editor.').format(info.search_str, this_rgb_path))\n",
    "            raise ValueError(message)\n",
    "    \n",
    "    if len(imgs) == 0:\n",
    "        message =(('No images found in any search directories specified. ' +\\\n",
    "                  'Check directory names, search strings, and file extensions' +\\\n",
    "                  'in parameters file.'))\n",
    "        raise RuntimeError(message)\n",
    "    else: \n",
    "        print('%s images found\\n\\nMaking director text file...\\n' % len(imgs))\n",
    "    \n",
    "    \n",
    "    # Write director and read it back in as a dataframe\n",
    "    director_path = os.path.join(out_dir,'director.txt')\n",
    "    \n",
    "    fade_frames = write_director(director_path, imgs, xy_txt, (rows,cols), m_per_frame, fps, fade_time) # write_director() returns the frames that need to be interpolated between\n",
    "    df_direct = read_txt(director_path)\n",
    "    \n",
    "    ######## Make extent indicator ########\n",
    "    extent_size = 0, 0\n",
    "    if 'legend_info' in inputs:\n",
    "        txt_footprint = int(rows * .15)\n",
    "    if extent_shp:\n",
    "        ds_coords, ds_extent = get_coords(extent_shp)\n",
    "        max_extent_size = int(max_extent_size)\n",
    "        pt_frequ = int(pt_frequ)\n",
    "        try: rot = float(extent_rotation)\n",
    "        except: rot = None\n",
    "    \n",
    "        pt_time = fps * float(pt_time)\n",
    "        pt_size = max_extent_size/40\n",
    "        radius = int(pt_size/2)\n",
    "        circle = np.zeros((2 * radius + 1, 2 * radius + 1))\n",
    "        y_c, x_c = np.ogrid[-radius : radius + 1, -radius : radius + 1]\n",
    "        pt_mask = x_c**2 + y_c**2 <= radius**2\n",
    "        if 'extent_pad' in inputs:\n",
    "            try:\n",
    "                if '.' in extent_pad: \n",
    "                    extent_pad = [float(extent_pad)] * 4\n",
    "                else:\n",
    "                    extent_pad = [str_to_number(i) for i in extent_pad.split(',')]\n",
    "            except:\n",
    "                raise ValueError('extent_pad not understood: %s' % extent_pad)\n",
    "        elif 'legend_info' in inputs:\n",
    "            extent_pad = [.05, txt_footprint/3, .05, .05]\n",
    "        else:\n",
    "            extent_pad = .05\n",
    "        base_ar, alpha_ar, offset = shp_to_array(ds_coords, ds_extent, max_extent_size, out_dir, pad=extent_pad)\n",
    "        extent_size = base_ar.shape\n",
    "        max_extent_size = max(extent_size)\n",
    "        ul_y, ul_x = max_extent_size - max_extent_size/10, extent_size[1]/2 \n",
    "        # If the \n",
    "        if len(np.unique(base_ar)) < 1:\n",
    "            sys.exit(('Could not read extent_shp shapefile provided:\\n%s' +\\\n",
    "                      '\\nPlease check the path in the master parameters file ' +\\\n",
    "                      'and that the shapefile is valid' % extent_shp))\n",
    "        # Get the extent indicator points\n",
    "        import pdb; pdb.set_trace()\n",
    "        df_extent = pd.DataFrame(df_direct[[(i % pt_frequ == 0) for i in df_direct.index]], columns=['x','y','x_off', 'y_off', 'alpha'])\n",
    "        #df_extent['alpha'] = 0.0 # Becomes numerator of alpha ratio\n",
    "        calc_extent_offsets(df_extent, ds_extent, pt_size, base_ar.shape, offset)\n",
    "        \n",
    "        \n",
    "    ##### Make legend(s) #######\n",
    "    if 'legend_info' in inputs:\n",
    "        if 'legend_position' in inputs:\n",
    "            extent_position = legend_position\n",
    "        if extent_shp: \n",
    "            legend_w = base_ar.shape[1] #extend ar size\n",
    "            legend_h = min(rows/2, int(rows - base_ar.shape[0] - txt_footprint))\n",
    "        else:\n",
    "            if 'legend_size' not in inputs:\n",
    "                legend_w = cols/4\n",
    "                legend_h = int(rows - txt_footprint)\n",
    "            else:\n",
    "                legend_h, legend_w = [int(s) for s in legend_size.split(',')]\n",
    "        legend_size = legend_h, legend_w\n",
    "        if 'label_font_size' in inputs:\n",
    "            label_font_size = str_to_number(label_font_size)\n",
    "        \n",
    "        legend_info = read_txt(legend_info)\n",
    "        legends = {}\n",
    "        legend_widths = []\n",
    "        for i, info in legend_info.loc[file_info.index].iterrows():\n",
    "            labels = {}\n",
    "            for label in info['labels'].split(','): #can't use info.labels\n",
    "                k, v = label.strip().split(':')\n",
    "                labels[int(k)] = v\n",
    "            labels = pd.Series(labels)\n",
    "            \n",
    "            legend, alpha = make_legend(legend_size, info.title, labels, info.color_mode, rgb_dics[info.search_str], font_path, font_size=label_font_size)\n",
    "            legend_widths.append(legend.shape[1])\n",
    "            legends[info.search_str] = legend, alpha\n",
    "            out_path = os.path.join(out_dir, 'legend_%s.png' % info.title)\n",
    "            cv2.imwrite(out_path, legend)\n",
    "            cv2.imwrite(out_path.replace('.png', '_alpha.png'), 255 - alpha * 255)\n",
    "\n",
    "        # Figure out legend position            \n",
    "        max_legend_w = max(legend_widths)\n",
    "        #if 'extent_shp' in inputs:\n",
    "        _, ext_inds, _ = get_overlay_inds(extent_position, (extent_size[0], max_legend_w), (rows, cols))\n",
    "        legend_ur = (ext_inds[0] - txt_footprint)/2 + txt_footprint - legend_h/2\n",
    "        legend_inds = legend_ur, legend_ur + legend_h, ext_inds[2], ext_inds[3]\n",
    "\n",
    "        extent_position = extent_position.lower()\\\n",
    "                            .replace('right','center')\\\n",
    "                            .replace('left', 'center')\n",
    "            #import pdb; pdb.set_trace()\n",
    "\n",
    "        #Make a dictionary with arrays of text for each year. Text is centered\n",
    "        #  in legend panel at the top\n",
    "        year_font_size = int(txt_footprint/3 * 1.467) # 1.467 to make font size == pixel size\n",
    "        ctr_x = legend_inds[2] + max_legend_w/2 \n",
    "        txt_dict, txt_size, txt_uly = make_text_dic(years, font_path, rows, cols, ctr_x=ctr_x, ul_y=txt_footprint/3, font_size=year_font_size)\n",
    "    \n",
    "    # Make text dict with text in bottom center of frame\n",
    "    else:\n",
    "        txt_dict, txt_size, txt_uly = make_text_dic(years, font_path, rows, cols)\n",
    "    \n",
    "    basepath = os.path.join(out_dir, 'frame')\n",
    "    df_direct['basepath'] = basepath\n",
    "    \n",
    "    \n",
    "    # If a shaded relief raster is specififed, open the dataset\n",
    "    if 'hillshade_path' in inputs: \n",
    "        try:\n",
    "            hs_opacity = int(hs_opacity) / 100.0\n",
    "        except:\n",
    "            print('Invalid or no opacity given for shaded relief... Using 15 %\\n')\n",
    "            hs_opacity = .15\n",
    "    \n",
    "    # Get the coordinates for the burn shapefile if burn_shp is specified in the params\n",
    "    burn_coords = False\n",
    "    burn_extent = False\n",
    "    if 'burn_shp' in locals():\n",
    "        burn_coords, burn_extent = get_coords(burn_shp)\n",
    "        if burn_coords == None:\n",
    "            print('Problem reading burn_shp')\n",
    "            print('Making frames wihtout burning in shapefile outline')\n",
    "    \n",
    "    # For each img, open the dataset and make all frames for that dataset\n",
    "    n_imgs = len(imgs)\n",
    "    for img_count, img_path in enumerate(sorted(df_direct.file.unique())):\n",
    "        t1 = time.time()\n",
    "        print('Making frames for:\\n{0}\\nProcessing img {1} out of {2}'.format(img_path, img_count + 1, n_imgs))\n",
    "        df_temp = df_direct[df_direct.file == img_path] #Get just records for this ds\n",
    "        df_temp['img_path'] = img_path\n",
    "        df_temp['bands'] = bands\n",
    "\n",
    "        args = []\n",
    "        ''' make progress bar per image and for whole animation'''\n",
    "        for ind, frame_info in df_temp.iterrows():\n",
    "            \n",
    "            if extent_shp:\n",
    "                \n",
    "                extent_info = [ds_extent, base_ar, pt_size, pt_mask, df_extent.iloc[:ind], pt_time, base_ar, alpha_ar, extent_position, rot]\n",
    "            args.append([frame_info,\n",
    "                         bands,\n",
    "                         rgb_dics, # Could get the specific dict here instead of passing all\n",
    "                         hillshade_path,\n",
    "                         hs_opacity,\n",
    "                         extent_shp,\n",
    "                         extent_info,\n",
    "                         burn_coords,\n",
    "                         burn_extent,\n",
    "                         txt_dict,\n",
    "                         txt_size,\n",
    "                         txt_uly,\n",
    "                         legends,\n",
    "                         legend_inds])\n",
    "        \n",
    "        # If predicting in parallel, make lists of args and set up a Pool of workers\n",
    "        if n_jobs:\n",
    "            n_jobs = int(n_jobs)\n",
    "            # pool = Pool(n_jobs)\n",
    "            pool = ThreadPool(n_jobs)\n",
    "            pool.map(par_make_frame, args, 1)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            \n",
    "        else:\n",
    "            for arg in args:\n",
    "                par_make_frame(arg)\n",
    "\n",
    "        print('Time for this image: %.1f minutes\\n' % ((time.time() - t1)/60))\n",
    "    \n",
    "#     # If there is a switch between image sets, fade where the change occurs\n",
    "#     frame_count = len(df_direct)\n",
    "#     nframes = fps * fade_time\n",
    "#     for frame in fade_frames:\n",
    "#         print('Interpolating for frames {0} to {1}'.format(int(frame), int(frame) + nframes))\n",
    "#         interpolate_imgs(frame, basepath, nframes)\n",
    "#         frame_count += nframes\n",
    "    \n",
    "#     if 'movie_basename' in locals():\n",
    "#         print('Writing movie file...')\n",
    "#         import movie_from_frames as movie\n",
    "#         out_movie = os.path.join(out_dir, movie_basename)\n",
    "#         movie.main(out_dir, fps, out_movie)\n",
    "        \n",
    "    \n",
    "#     print('{0} frames written to:\\n{1}'.format(frame_count, out_dir))\n",
    "#     print('\\nTotal runtime: %.1f minutes\\n' % ((time.time() - t0)/60))\n",
    "#     return out_dir#, df_extent'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e26c97b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters read from:\n",
      "/Users/yingtong/Codes/EE840/Animation/params/master_params.txt\n",
      "\n",
      "20 images found\n",
      "\n",
      "Making director text file...\n",
      "\n",
      "   x_start  y_start   x_end   y_end  year_start  year_end  band_start  \\\n",
      "0   194509   908000  194509  908000        2001      2020           1   \n",
      "1   194509   908000  194509  908000        2001      2020           1   \n",
      "2   194509   908000  232800  866000        2001      2020           1   \n",
      "3   232800   866000  232800  866000        2001      2020           1   \n",
      "\n",
      "       zoom  height_scale  speed_scale acceleration search_string  \\\n",
      "0     start           2.0          1.0         none    Mass_LCMAP   \n",
      "1      none           0.0          1.0         none    Mass_LCMAP   \n",
      "2  startend           0.5          0.4         none    Mass_LCMAP   \n",
      "3      none           0.0          1.0         none    Mass_LCMAP   \n",
      "\n",
      "  replace_method  nframes  scenes_per_sec  by_file  hillshade  \n",
      "0              s      300               2        1          0  \n",
      "1              s      600               1        1          0  \n",
      "2              s      300               2        1          0  \n",
      "3              s      600               1        1          0  \n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2001.tif\n",
      "Processing img 1 out of 20\n",
      "Time for this image: 1.7 minutes\n",
      "\n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2002.tif\n",
      "Processing img 2 out of 20\n",
      "Time for this image: 1.6 minutes\n",
      "\n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2003.tif\n",
      "Processing img 3 out of 20\n",
      "Time for this image: 1.6 minutes\n",
      "\n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2004.tif\n",
      "Processing img 4 out of 20\n",
      "Time for this image: 1.6 minutes\n",
      "\n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2005.tif\n",
      "Processing img 5 out of 20\n",
      "Time for this image: 1.6 minutes\n",
      "\n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2006.tif\n",
      "Processing img 6 out of 20\n",
      "Time for this image: 1.5 minutes\n",
      "\n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2007.tif\n",
      "Processing img 7 out of 20\n",
      "Time for this image: 1.5 minutes\n",
      "\n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2008.tif\n",
      "Processing img 8 out of 20\n",
      "Time for this image: 1.5 minutes\n",
      "\n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2009.tif\n",
      "Processing img 9 out of 20\n",
      "Time for this image: 1.3 minutes\n",
      "\n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2010.tif\n",
      "Processing img 10 out of 20\n",
      "Time for this image: 1.3 minutes\n",
      "\n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2011.tif\n",
      "Processing img 11 out of 20\n",
      "Time for this image: 1.2 minutes\n",
      "\n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2012.tif\n",
      "Processing img 12 out of 20\n",
      "Time for this image: 1.1 minutes\n",
      "\n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2013.tif\n",
      "Processing img 13 out of 20\n",
      "Time for this image: 1.1 minutes\n",
      "\n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2014.tif\n",
      "Processing img 14 out of 20\n",
      "Time for this image: 1.0 minutes\n",
      "\n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2015.tif\n",
      "Processing img 15 out of 20\n",
      "Time for this image: 0.9 minutes\n",
      "\n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2016.tif\n",
      "Processing img 16 out of 20\n",
      "Time for this image: 0.9 minutes\n",
      "\n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2017.tif\n",
      "Processing img 17 out of 20\n",
      "Time for this image: 0.9 minutes\n",
      "\n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2018.tif\n",
      "Processing img 18 out of 20\n",
      "Time for this image: 0.9 minutes\n",
      "\n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2019.tif\n",
      "Processing img 19 out of 20\n",
      "Time for this image: 0.8 minutes\n",
      "\n",
      "Making frames for:\n",
      "/Volumes/GoogleDrive/My Drive/PhD/Courses/TF/Data/image/Mass_LCMAP_2020.tif\n",
      "Processing img 20 out of 20\n",
      "Time for this image: 1.0 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #params = sys.argv[1]\n",
    "    #sys.exit(main(params))#'''\n",
    "    params = r'/Users/yingtong/Codes/EE840/Animation/params/master_params.txt'\n",
    "    main(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d307d59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d309926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add5f5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e0abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
